import gensim

from utils.embeddings import get_w2v_embeddings_from_path, get_w2v_embeddings_from_model
from Implementation.code_vulnerability_detection.dataloader import VulnerabilityDataloader
from Models.transformer import TransormerClassifierModel

import pandas as pd
import numpy as np


def train_validate_test_split(data, train_percent=.6, validate_percent=.2, seed=None):
    np.random.seed(seed)
    perm = np.random.permutation(data.index)
    m = len(data.index)
    train_end = int(train_percent * m)
    validate_end = int(validate_percent * m) + train_end
    train = data.iloc[perm[:train_end]]
    validate = data.iloc[perm[train_end:validate_end]]
    test = data.iloc[perm[validate_end:]]
    return train, test, validate


# # Load data loader
# dataloader = VulnerabilityDataloader('data/sample_index.csv','data/sample_index.csv',
#                                      batch_size=100, eval_portion=0.2, max_length=207)

dataframe = pd.read_csv('data/sample_index.csv')

# dataframe = pd.read_csv('../../dataset/juliet_java-file-metrics.csv')
# dataloader = VulnerabilityDataloader(dataframe, dataframe,
#                                      batch_size=100, eval_portion=0.2, max_length=207)

# max_length = len(max(dataframe.loc[:, "indexes"], key=len))

train_percent = .6
validate_percent = .2
seed = 42
train_set, test_set, validate_set = train_validate_test_split(dataframe, train_percent, validate_percent, seed=seed)
max_length = 207

dataloader = VulnerabilityDataloader(train_set=train_set, test_set=test_set, validate_set=validate_set,
                                     max_length=max_length)
# get word2vec embedding list
w2v_model_path = 'data/word2vec.model'
word2vec = gensim.models.Word2Vec.load(w2v_model_path)
embedding, vector_length = get_w2v_embeddings_from_model(word2vec, padding=True)

# embedding, vector_length = get_w2v_embeddings_from_path('data/word2vec.model', padding=True)

save_name = 'vul'
max_length = dataloader.get_max_length()

import itertools
vocab = set(list(itertools.chain.from_iterable(list(dataframe['source_code']))))
vocab = {k: v for v, k in enumerate(vocab)}
vocab_size =len(vocab)
d_model = 128

# Implement model
model = TransormerClassifierModel('models/' + save_name, 'logs/' + save_name, embedding=embedding,
                                  d_features=100, d_meta=None, max_length=max_length, d_classifier=256, n_classes=2,
                                  n_layers=6, n_head=8, dropout=0.1, use_bottleneck=True, d_bottleneck=128,
                                  vocab_size=vocab_size, d_model=d_model)

# Training
model.train(max_epoch=400, train_dataloader=dataloader.train_dataloader(), eval_dataloader=dataloader.val_dataloader(),
            device='cpu', save_mode='best', smoothing=False, earlystop=False)
# Evaluation

pred, real = model.get_predictions(dataloader.test_dataloader(), 'cpu')
pred_ = np.array(pred)[:, 1]
real = np.array(real).astype(int)
from utils.plot_curves import precision_recall, plot_pr_curve

area, precisions, recalls, thresholds = precision_recall(pred_, real)
plot_pr_curve(recalls, precisions, auc=area)

from utils.plot_curves import auc_roc, plot_roc_curve

auc, fprs, tprs, thresholds = auc_roc(pred_, real)
plot_roc_curve(fprs, tprs, auc)

from Implementation.ciena.metrics import results

df = results(real, np.array(pred).argmax(axis=-1), 0.5)
