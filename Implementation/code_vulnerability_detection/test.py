import gensim

from utils.embeddings import get_w2v_embeddings_from_path, get_w2v_embeddings_from_model
from Implementation.code_vulnerability_detection.dataloader import VulnerabilityDataloader
from Models.transformer import TransormerClassifierModel

import pandas as pd
import numpy as np


def train_validate_test_split(data, train_percent=.6, validate_percent=.2, seed=None):
    np.random.seed(seed)
    perm = np.random.permutation(data.index)
    m = len(data.index)
    train_end = int(train_percent * m)
    validate_end = int(validate_percent * m) + train_end
    train = data.iloc[perm[:train_end]]
    validate = data.iloc[perm[train_end:validate_end]]
    test = data.iloc[perm[validate_end:]]
    return train, test, validate


# Load data loader
dataframe = pd.read_csv('../../dataset/juliet_java-file-metrics.csv')[:50]
# max_length = len(max(dataframe.loc[:, "indexes"], key=len))

train_percent = .6
validate_percent = .2
seed = 42
train_set, test_set, validate_set = train_validate_test_split(dataframe, train_percent, validate_percent, seed=seed)
max_length = 100

save_name = 'vul'

import itertools

vocab = set(list(itertools.chain.from_iterable(list(dataframe['source_code']))))
vocab = {k: v for v, k in enumerate(vocab)}
vocab_size = len(vocab) + 1
d_model = 10
print(vocab_size)
# lf;msd

dataloader = VulnerabilityDataloader(train_set=train_set, test_set=test_set, validate_set=validate_set,
                                     max_length=max_length, vocab=vocab, batch_size=10)
max_length = dataloader.get_max_length()
print(max_length)

# Implement model
model = TransormerClassifierModel('models/' + save_name, 'logs/' + save_name, d_meta=None, max_length=max_length,
                                  d_classifier=256, n_classes=2,
                                  n_layers=6, n_head=8, dropout=0.1, use_bottleneck=True, d_bottleneck=128,
                                  vocab_size=vocab_size, d_model=d_model)
# Training
model.train(max_epoch=5, train_dataloader=dataloader.train_dataloader(), eval_dataloader=dataloader.val_dataloader(),
            device='cpu', save_mode='best', smoothing=False, earlystop=False)
# Evaluation

pred, real = model.get_predictions(dataloader.test_dataloader(), 'cpu')
pred_ = np.array(pred)[:, 1]
real = np.array(real).astype(int)
from utils.plot_curves import precision_recall, plot_pr_curve

area, precisions, recalls, thresholds = precision_recall(pred_, real)
print(area, precisions, recalls, thresholds)
plot_pr_curve(recalls, precisions, auc=area)

from utils.plot_curves import auc_roc, plot_roc_curve

auc, fprs, tprs, thresholds = auc_roc(pred_, real)
print(auc, fprs, tprs, thresholds)
plot_roc_curve(fprs, tprs, auc)

from Implementation.ciena.metrics import results

df = results(real, np.array(pred).argmax(axis=-1), 0.5)
print(df)

import sklearn

predicted_prob = pred  # np.array(pred)[:, 1]
y_test = np.array(real).astype(int)
predicted = np.argmax(predicted_prob, axis=1)
confusion = sklearn.metrics.confusion_matrix(y_true=y_test, y_pred=predicted)
print(confusion)
tn, fp, fn, tp = confusion.ravel()
print('\nTP:', tp)
print('FP:', fp)
print('TN:', tn)
print('FN:', fn)

## Performance measure
print('\nAccuracy: ' + str(sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predicted)))
print('Precision: ' + str(sklearn.metrics.precision_score(y_true=y_test, y_pred=predicted)))
print('Recall: ' + str(sklearn.metrics.recall_score(y_true=y_test, y_pred=predicted)))
print('F-measure: ' + str(sklearn.metrics.f1_score(y_true=y_test, y_pred=predicted)))
print("False Positive Rate:" + str(fp / (tn + fp)))
print('AUC: ' + str(sklearn.metrics.roc_auc_score(y_true=y_test, y_score=np.argmax(predicted_prob, axis=1))))
print('Precision-Recall AUC: ' + str(
    sklearn.metrics.average_precision_score(y_true=y_test, y_score=np.argmax(predicted_prob, axis=1))))
print('MCC: ' + str(sklearn.metrics.matthews_corrcoef(y_true=y_test, y_pred=predicted)))
