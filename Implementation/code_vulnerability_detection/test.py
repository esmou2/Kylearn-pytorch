import gensim

from utils.embeddings import get_w2v_embeddings_from_path, get_w2v_embeddings_from_model
from Implementation.code_vulnerability_detection.dataloader import VulnerabilityDataloader
from Models.transformer import TransormerClassifierModel

import pandas as pd
import numpy as np


def train_validate_test_split(data, train_percent=.6, validate_percent=.2, seed=None):
    np.random.seed(seed)
    perm = np.random.permutation(data.index)
    m = len(data.index)
    train_end = int(train_percent * m)
    validate_end = int(validate_percent * m) + train_end
    train = data.iloc[perm[:train_end]]
    validate = data.iloc[perm[train_end:validate_end]]
    test = data.iloc[perm[validate_end:]]
    return train, test, validate

# Load data loader
dataframe = pd.read_csv('../../dataset/juliet_java-file-metrics.csv')[:100]
# max_length = len(max(dataframe.loc[:, "indexes"], key=len))

train_percent = .6
validate_percent = .2
seed = 42
train_set, test_set, validate_set = train_validate_test_split(dataframe, train_percent, validate_percent, seed=seed)
max_length = 100

save_name = 'vul'

import itertools
vocab = set(list(itertools.chain.from_iterable(list(dataframe['source_code']))))
vocab = {k: v for v, k in enumerate(vocab)}
vocab_size =len(vocab) + 1
d_model = 128
print(vocab_size)
# lf;msd

dataloader = VulnerabilityDataloader(train_set=train_set, test_set=test_set, validate_set=validate_set,
                                     max_length=max_length, vocab=vocab, batch_size=10)
max_length = dataloader.get_max_length()
print(max_length)
# Implement model
model = TransormerClassifierModel('models/' + save_name, 'logs/' + save_name, d_meta=None, max_length=max_length, d_classifier=256, n_classes=2,
                                  n_layers=6, n_head=8, dropout=0.1, use_bottleneck=True, d_bottleneck=128,
                                  vocab_size=vocab_size, d_model=d_model)
# Training
model.train(max_epoch=5, train_dataloader=dataloader.train_dataloader(), eval_dataloader=dataloader.val_dataloader(),
            device='cpu', save_mode='best', smoothing=False, earlystop=False)
# Evaluation

pred, real = model.get_predictions(dataloader.test_dataloader(), 'cpu')
pred_ = np.array(pred)[:, 1]
real = np.array(real).astype(int)
from utils.plot_curves import precision_recall, plot_pr_curve

area, precisions, recalls, thresholds = precision_recall(pred_, real)
plot_pr_curve(recalls, precisions, auc=area)

from utils.plot_curves import auc_roc, plot_roc_curve

auc, fprs, tprs, thresholds = auc_roc(pred_, real)
plot_roc_curve(fprs, tprs, auc)

from Implementation.ciena.metrics import results

df = results(real, np.array(pred).argmax(axis=-1), 0.5)
