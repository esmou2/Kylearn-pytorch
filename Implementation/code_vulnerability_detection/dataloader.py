import numpy as np
from Dataloader.transformer import *
from torch.utils.data.sampler import SubsetRandomSampler


class VulnerabilityDataset(Dataset):
    def __init__(self, dataframe, max_length=None):
        super().__init__()
        # Load training data
        # dataframe = pd.read_csv(dataframe)

        self.input_sequence_index, self.position_index, max_length = process_indexes(dataframe['indexes'], max_length)
        self.targets = dataframe['is_vulnerable'].values.reshape(-1, 1)
        self.max_length = max_length

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, index):
        if torch.is_tensor(index):
            index = index.tolist()
        sample = (torch.from_numpy(self.input_sequence_index[index, :]),
                  torch.from_numpy(self.position_index[index, :]),
                  torch.from_numpy(self.targets[index, :]))

        return sample


def train_validate_test_split(data, train_percent=.6, validate_percent=.2, seed=None):
    np.random.seed(seed)
    perm = np.random.permutation(data.index)
    m = len(data.index)
    train_end = int(train_percent * m)
    validate_end = int(validate_percent * m) + train_end
    train = data.iloc[perm[:train_end]]
    validate = data.iloc[perm[train_end:validate_end]]
    test = data.iloc[perm[validate_end:]]
    return train, validate, test


class VulnerabilityDataloader():
    # def __init__(self, train_path, test_path, batch_size, eval_portion, max_length=207, shuffle=True):
    #     self.max_length = max_length
    #
    #     # train_set = VulnerabilityDataset(train_path, max_length=max_length)
    #     # test_set = VulnerabilityDataset(test_path, max_length=max_length)
    #
    #     train_set = VulnerabilityDataset(train_path, max_length=max_length)
    #     test_set = VulnerabilityDataset(test_path, max_length=max_length)
    #
    #     train_size = len(train_set)
    #     train_indices = list(range(train_size))
    #
    #     test_size = len(test_set)
    #     test_indices = list(range(test_size))
    #     split = int(np.floor(eval_portion * test_size))
    #
    #     if shuffle:
    #         np.random.seed(42)
    #         np.random.shuffle(test_indices)
    #
    #     test_indices, eval_indices = test_indices[split:], test_indices[:split]
    #
    #     train_sampler = SubsetRandomSampler(train_indices)
    #     # train_sampler = BalanceSampler(train_set.targets, train_indices)
    #     valid_sampler = SubsetRandomSampler(eval_indices)
    #
    #     self.train_loader = DataLoader(train_set, batch_size, num_workers=4)
    #     self.val_loader = DataLoader(train_set, batch_size, num_workers=4)
    #     self.test_loader = DataLoader(test_set, batch_size, num_workers=4)

    def __init__(self, data, column_name, batch_size=100, train_percent=.2, validate_percent=.2, max_length=None, shuffle=True, num_workers=4):

        if not max_length:
            max_length = len(max(data.loc[:, column_name], key=len))
        self.max_length = max_length

        seed = None
        if shuffle:
            seed = 42
        train_set, validate_set, test_set = train_validate_test_split(data, train_percent, validate_percent, seed=seed)

        train_set = VulnerabilityDataset(train_set, max_length)
        test_set = VulnerabilityDataset(test_set, max_length)
        validate_set = VulnerabilityDataset(validate_set, max_length)

        self.train_loader = DataLoader(train_set, batch_size, num_workers=num_workers)
        self.val_loader = DataLoader(validate_set, batch_size, num_workers=num_workers)
        self.test_loader = DataLoader(test_set, batch_size, num_workers=num_workers)

    def train_dataloader(self):
        return self.train_loader

    def val_dataloader(self):
        return self.val_loader

    def test_dataloader(self):
        return self.test_loader

    def get_max_length(self):
        return self.max_length
